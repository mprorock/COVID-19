{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "# for geocoding stuff\n",
    "import geopandas as gpd\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./data/\"\n",
    "input_dir = \"./csse_covid_19_data/csse_covid_19_daily_reports/\"\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob(input_dir+'*.{}'.format(extension))]\n",
    "\n",
    "# %% combine em up\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "combined_csv.to_csv(output_dir + \"covid_19_raw.csv\", index=False, encoding='utf-8-sig') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_timeseries_by_group(df, group_col='Location'):\n",
    "    tdf = df.copy()\n",
    "    tdf=tdf.sort_values('Last Update')\n",
    "    timeseries_by_location = tdf.groupby(group_col)\n",
    "    \n",
    "    for days_shift in [1,3,7]:\n",
    "        for orig_column in ['Recovered', 'Deaths', 'Confirmed', 'Active']:\n",
    "            tdf[f'{days_shift}d new {orig_column}'] = timeseries_by_location[orig_column].diff(periods=days_shift)\n",
    "\n",
    "    day_location_reached_100 = tdf[tdf['Confirmed']>100].groupby(group_col)['Last Update'].min().to_dict()\n",
    "    day_location_reached_100_active = tdf[tdf['Active']>100].groupby(group_col)['Last Update'].min().to_dict()\n",
    "    \n",
    "    day_location_reached_1_deaths = tdf[tdf['Deaths']>1].groupby(group_col)['Last Update'].min().to_dict()\n",
    "    day_location_reached_10_deaths = tdf[tdf['Deaths']>10].groupby(group_col)['Last Update'].min().to_dict()\n",
    "    day_location_reached_100_deaths = tdf[tdf['Deaths']>100].groupby(group_col)['Last Update'].min().to_dict()\n",
    "    #day_location_reached_1_per_100k = tdf[tdf['Confirmed per 100k capita']>1].groupby(group_col)['Last Update'].min().to_dict()\n",
    "\n",
    "    def shift_dates(row, offset_by_location):\n",
    "        date = row['Last Update']\n",
    "        location = row[group_col]\n",
    "        if location in offset_by_location:\n",
    "            return int((date - offset_by_location[location]) / pd.Timedelta(days=1))\n",
    "\n",
    "    tdf['days since 100 cases - ' + group_col] = tdf.apply(\n",
    "        shift_dates,\n",
    "        offset_by_location=day_location_reached_100,\n",
    "        axis='columns'\n",
    "    )\n",
    "    tdf['days since 100 active - ' + group_col] = tdf.apply(\n",
    "        shift_dates,\n",
    "        offset_by_location=day_location_reached_100_active,\n",
    "        axis='columns'\n",
    "    )\n",
    "    \n",
    "    tdf['days since 1 deaths - ' + group_col] = tdf.apply(\n",
    "        shift_dates,\n",
    "        offset_by_location=day_location_reached_1_deaths,\n",
    "        axis='columns'\n",
    "    )\n",
    "    tdf['days since 10 deaths - ' + group_col] = tdf.apply(\n",
    "        shift_dates,\n",
    "        offset_by_location=day_location_reached_10_deaths,\n",
    "        axis='columns'\n",
    "    )\n",
    "    tdf['days since 100 deaths - ' + group_col] = tdf.apply(\n",
    "        shift_dates,\n",
    "        offset_by_location=day_location_reached_100_deaths,\n",
    "        axis='columns'\n",
    "    )\n",
    "#     tdf['days since 1 case/100k people - ' + group_col] = tdf.apply(\n",
    "#         shift_dates,\n",
    "#         offset_by_location=day_location_reached_1_per_100k,\n",
    "#         axis='columns'\n",
    "#     )\n",
    "    return tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geopy.geocoders.options.default_timeout = 30\n",
    "locator = Nominatim(user_agent=\"mesur.io\")\n",
    "geocode = RateLimiter(locator.geocode, min_delay_seconds=1)\n",
    "revgeocode = RateLimiter(locator.reverse, min_delay_seconds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do a little data cleanup\n",
    "combined_csv['Country/Region'] = combined_csv['Country/Region'].str.strip()\n",
    "combined_csv['Country/Region'] = combined_csv['Country/Region'].replace('Korea, South', 'South Korea')\n",
    "combined_csv['Country/Region'] = combined_csv['Country/Region'].replace('Republic of Korea', 'South Korea')\n",
    "combined_csv['Country/Region'] = combined_csv['Country/Region'].replace('Iran (Islamic Republic of)', 'Iran')\n",
    "combined_csv['Country/Region'] = combined_csv['Country/Region'].replace('Mainland China', 'China')\n",
    "combined_csv['Country_Region'] = combined_csv['Country_Region'].replace('United Kingdom', 'UK')\n",
    "\n",
    "combined_csv['Country_Region'] = combined_csv['Country_Region'].replace('*', '').replace(', The', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined_csv.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = combined.groupby(['Country/Region', 'Province/State'])['Latitude', 'Longitude'].mean().reset_index()\n",
    "locations.columns = ['Country/Region', 'Province/State', 'Latitude_Lookup', 'Longitude_Lookup']\n",
    "\n",
    "combined = pd.merge(left=combined, right=locations, left_on=['Country/Region', 'Province/State'], right_on=['Country/Region', 'Province/State'], how='left')\n",
    "combined['Latitude'] = combined['Latitude'].fillna(combined['Latitude_Lookup'])\n",
    "combined['Longitude'] = combined['Longitude'].fillna(combined['Longitude_Lookup'])\n",
    "del combined['Latitude_Lookup'] \n",
    "del combined['Longitude_Lookup'] \n",
    "\n",
    "locations2 = combined.groupby(['Country/Region', 'Province/State'])['Lat', 'Long_'].mean().reset_index()\n",
    "locations2.columns = ['Country/Region', 'Province/State', 'Latitude_Lookup', 'Longitude_Lookup']\n",
    "\n",
    "combined = pd.merge(left=combined, right=locations2, left_on=['Country/Region', 'Province/State'], right_on=['Country/Region', 'Province/State'], how='left')\n",
    "combined['Latitude'] = combined['Latitude'].fillna(combined['Latitude_Lookup'])\n",
    "combined['Longitude'] = combined['Longitude'].fillna(combined['Longitude_Lookup'])\n",
    "del combined['Latitude_Lookup'] \n",
    "del combined['Longitude_Lookup'] \n",
    "\n",
    "combined['Last Update'] = combined['Last Update'].fillna(combined['Last_Update'])\n",
    "combined['Country/Region'] = combined['Country/Region'].fillna(combined['Country_Region'])\n",
    "combined['Province/State'] = combined['Province/State'].fillna(combined['Province_State'])\n",
    "combined['Province/State'] = combined['Province/State'].fillna(combined['Country/Region'])\n",
    "combined['Confirmed'] = combined['Confirmed'].fillna(0)\n",
    "combined['Deaths'] = combined['Deaths'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['Last Update'] = pd.to_datetime(combined['Last Update']).dt.round(freq = 'D')\n",
    "combined = combined.sort_values('Last Update').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['Geo_Input'] = combined['Province/State']+', '+combined['Country/Region'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_located = combined[combined['Latitude'].isna()]\n",
    "non_located = non_located[non_located['Province/State'] != 'Cruise Ship']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_inputs = non_located['Geo_Input'].unique()\n",
    "combined['Location_Key_Raw'] = combined.apply(lambda x: (x.Latitude, x.Longitude), axis = 1)\n",
    "#for testing you may want to trim this down a bit\n",
    "#geo_inputs = geo_inputs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode_jh():\n",
    "    print('Geocoding for: ', len(geo_inputs), 'locations')\n",
    "    #use progress_apply() for interactive progress\n",
    "    d = dict(zip(geo_inputs, pd.Series(geo_inputs).apply(geocode).apply(lambda x: (x.latitude if pd.notnull(x.latitude) else x.latitude, \n",
    "                                                                                   x.longitude if pd.notnull(x.longitude) else x.longitude) if pd.notnull(x) else x)\n",
    "                )\n",
    "            )\n",
    "    pickle.dump(d, open('./reference/geolod_dict.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geocode_jh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pickle.load(open('./reference/geolod_dict.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['Location_Key'] = combined['Geo_Input'].map(d)\n",
    "combined['Location_Key'] = combined['Location_Key'].fillna(combined['Location_Key_Raw'])\n",
    "combined['Latitude'] = combined.loc[combined['Latitude'].isna(), 'Location_Key'].apply(lambda x: x[0])\n",
    "combined['Longitude'] = combined.loc[combined['Longitude'].isna(), 'Location_Key'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's do a recovery est\n",
    "# first need the day of outbreak\n",
    "combined = combined.sort_values('Last Update').reset_index(drop=True)\n",
    "combined['Day'] = combined.groupby('Country/Region').cumcount()\n",
    "combined['DayLoc'] = combined.groupby(['Latitude','Longitude']).cumcount()\n",
    "combined['DayCountry'] = combined.groupby('Country/Region').cumcount()\n",
    "combined['DayCountryProvince'] = combined.groupby('Geo_Input').cumcount()\n",
    "\n",
    "combined['UnknownActive'] = combined['Confirmed'] - combined['Deaths']\n",
    "combined['RecoveredEst'] = np.floor(combined['UnknownActive'] * .14)\n",
    "combined['Recovered'] = combined['Recovered'].fillna(0)\n",
    "#hold on this for now, there is a formula that is curve based for this\n",
    "#combined.loc[combined['Recovered'] == 0, 'Recovered'] = combined['RecoveredEst']\n",
    "combined['Active'] = combined['UnknownActive'] - combined['Recovered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a little reording and subselection\n",
    "combined_csv = combined[['Last Update','Latitude','Longitude','Country/Region','Province/State','FIPS','Admin2',\n",
    "                         'Confirmed','Deaths','Recovered','UnknownActive', 'Active',\n",
    "                         'Day','DayLoc','DayCountry','DayCountryProvince']]\n",
    "combined_csv = combined_csv.sort_values(['Last Update','Latitude','Longitude','Country/Region','Province/State'])\n",
    "#combined_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_country_jh(row):\n",
    "    location_segments = [\n",
    "        row['Province/State'], row['Country/Region']\n",
    "    ]\n",
    "    cleaned_location_segments = [\n",
    "        segment\n",
    "        for segment in location_segments\n",
    "        if type(segment) is str\n",
    "    ]\n",
    "    return ', '.join(cleaned_location_segments)\n",
    "\n",
    "combined_csv['State and Country'] = combined_csv.apply(get_state_country_jh, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv = calc_timeseries_by_group(combined_csv, 'Country/Region')\n",
    "combined_csv = calc_timeseries_by_group(combined_csv, 'State and Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv.to_csv(output_dir + \"combined.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_cases = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/web-data/data/cases.csv')\n",
    "web_cases_state = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/web-data/data/cases_state.csv')\n",
    "web_cases_country = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/web-data/data/cases_country.csv')\n",
    "web_cases_time = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/web-data/data/cases_time.csv')\n",
    "\n",
    "web_cases.to_csv(output_dir + \"web_cases.csv\", index=False, encoding='utf-8-sig')\n",
    "web_cases_state.to_csv(output_dir + \"web_cases_state.csv\", index=False, encoding='utf-8-sig')\n",
    "web_cases_country.to_csv(output_dir + \"web_cases_country.csv\", index=False, encoding='utf-8-sig')\n",
    "web_cases_time.to_csv(output_dir + \"web_cases_time.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_csv.copy()\n",
    "df['Last Update'] = pd.to_datetime(df['Last Update']).dt.round(freq = 'D')\n",
    "\n",
    "def firsti(Series, offset):\n",
    "    return Series.first(offset)\n",
    "\n",
    "df = df.groupby(by=['Last Update', 'Country/Region'])[\n",
    "    #'Last Update', 'Country/Region',\n",
    "    'Confirmed', 'Deaths', 'Recovered'].sum().reset_index()\n",
    "df = df.sort_values('Last Update', ascending=True).reset_index()\n",
    "df['Active Cases'] = df['Confirmed'] - df['Recovered'] - df['Deaths']\n",
    "df['Cases'] = df['Confirmed'] - df['Recovered'] \n",
    "df['Death Rate'] = df['Deaths'] / df['Confirmed']\n",
    "df['Recovery Rate'] = df['Recovered'] / df['Confirmed']\n",
    "df['New Deaths'] = df['Deaths'] - df['Deaths'].shift()\n",
    "df['New Recovered'] = df['Recovered'] - df['Recovered'].shift()\n",
    "df['New Cases'] = df['Confirmed'] - df['Confirmed'].shift()\n",
    "df['New Case Rate'] = df['New Cases'].pct_change()\n",
    "df['New Death Rate'] = df['New Deaths'].pct_change()\n",
    "df['Last Update'] = pd.to_datetime(df['Last Update'])\n",
    "df['Date'] = pd.DatetimeIndex(df['Last Update']).astype ( np.int64 )/1000000\n",
    "df['Day'] = df.groupby('Country/Region').cumcount()\n",
    "df = df.dropna().reset_index()\n",
    "\n",
    "df.to_csv(output_dir + \"covid_19_by_date_and_country.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf = df.copy().groupby('Last Update').agg({\n",
    "    'Confirmed':'sum',\n",
    "    'Deaths':'sum',\n",
    "    'Recovered':'sum'\n",
    "    }).reset_index()\n",
    "overallDf = overallDf.sort_values('Last Update', ascending=True)\n",
    "overallDf['Active Cases'] = overallDf['Confirmed'] - overallDf['Recovered'] - overallDf['Deaths']\n",
    "overallDf['Cases'] = overallDf['Confirmed'] - overallDf['Recovered'] \n",
    "overallDf['Death Rate'] = overallDf['Deaths'] / overallDf['Confirmed']\n",
    "overallDf['Recovery Rate'] = overallDf['Recovered'] / overallDf['Confirmed']\n",
    "overallDf['New Deaths'] = overallDf['Deaths'] - overallDf['Deaths'].shift()\n",
    "overallDf['New Recovered'] = overallDf['Recovered'] - overallDf['Recovered'].shift()\n",
    "overallDf['New Cases'] = overallDf['Confirmed'] - overallDf['Confirmed'].shift()\n",
    "overallDf['New Case Rate'] = overallDf['New Cases'].pct_change()\n",
    "overallDf['New Death Rate'] = overallDf['New Deaths'].pct_change()\n",
    "overallDf['Date'] = pd.DatetimeIndex(overallDf['Last Update']).astype ( np.int64 )/1000000\n",
    "overallDf = overallDf.dropna().reset_index()\n",
    "\n",
    "overallDf.to_csv(output_dir + \"covid_19_by_date.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf = df.copy().groupby('Day').agg({\n",
    "    'Confirmed':'sum',\n",
    "    'Deaths':'sum',\n",
    "    'Recovered':'sum'\n",
    "    }).reset_index()\n",
    "overallDf = overallDf.sort_values('Day', ascending=True)\n",
    "overallDf['Active Cases'] = overallDf['Confirmed'] - overallDf['Recovered'] - overallDf['Deaths']\n",
    "overallDf['Cases'] = overallDf['Confirmed'] - overallDf['Recovered'] \n",
    "overallDf['Death Rate'] = overallDf['Deaths'] / overallDf['Confirmed']\n",
    "overallDf['Recovery Rate'] = overallDf['Recovered'] / overallDf['Confirmed']\n",
    "overallDf['New Deaths'] = overallDf['Deaths'] - overallDf['Deaths'].shift()\n",
    "overallDf['New Recovered'] = overallDf['Recovered'] - overallDf['Recovered'].shift()\n",
    "overallDf['New Cases'] = overallDf['Confirmed'] - overallDf['Confirmed'].shift()\n",
    "overallDf['New Case Rate'] = overallDf['New Cases'].pct_change()\n",
    "overallDf['New Death Rate'] = overallDf['New Deaths'].pct_change()\n",
    "overallDf = overallDf.dropna().reset_index()\n",
    "\n",
    "overallDf.to_csv(output_dir + \"covid_19_by_day.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf = df.copy().groupby(by=['Country/Region','Day']).agg({\n",
    "    'Confirmed':'sum',\n",
    "    'Deaths':'sum',\n",
    "    'Recovered':'sum'\n",
    "    }).reset_index()\n",
    "overallDf = overallDf.sort_values('Day', ascending=True)\n",
    "overallDf['Active Cases'] = overallDf['Confirmed'] - overallDf['Recovered'] - overallDf['Deaths']\n",
    "overallDf['Cases'] = overallDf['Confirmed'] - overallDf['Recovered'] \n",
    "overallDf['Death Rate'] = overallDf['Deaths'] / overallDf['Confirmed']\n",
    "overallDf['Recovery Rate'] = overallDf['Recovered'] / overallDf['Confirmed']\n",
    "overallDf['New Deaths'] = overallDf['Deaths'] - overallDf['Deaths'].shift()\n",
    "overallDf['New Recovered'] = overallDf['Recovered'] - overallDf['Recovered'].shift()\n",
    "overallDf['New Cases'] = overallDf['Confirmed'] - overallDf['Confirmed'].shift()\n",
    "overallDf['New Case Rate'] = overallDf['New Cases'].pct_change()\n",
    "overallDf['New Death Rate'] = overallDf['New Deaths'].pct_change()\n",
    "overallDf = overallDf.dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#john's hopkins raw files\n",
    "ts_deaths = pd.read_csv('csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')\n",
    "ts_confirmed = pd.read_csv('csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n",
    "#new recovered tracking has now been dropped :(\n",
    "ts_recovered = pd.read_csv('csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv')\n",
    "\n",
    "# let's unpivot that nasty excel style stuff\n",
    "ts_deaths = pd.melt(ts_deaths, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], var_name='Date', value_name='Observation')\n",
    "ts_deaths['Observation Type'] = 'Death'\n",
    "ts_confirmed = pd.melt(ts_confirmed, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], var_name='Date', value_name='Observation')\n",
    "ts_confirmed['Observation Type'] = 'Confirmed'\n",
    "ts_recovered = pd.melt(ts_recovered, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], var_name='Date', value_name='Observation')\n",
    "ts_recovered['Observation Type'] = 'Recovered'\n",
    "\n",
    "ts_deaths['Date'] = pd.to_datetime(ts_deaths['Date']).dt.round(freq = 'D')\n",
    "ts_confirmed['Date'] = pd.to_datetime(ts_confirmed['Date']).dt.round(freq = 'D')\n",
    "ts_recovered['Date'] = pd.to_datetime(ts_recovered['Date']).dt.round(freq = 'D')\n",
    "\n",
    "#and concat into one nice set\n",
    "covid_19_ts = ts_deaths.copy()\n",
    "covid_19_ts = covid_19_ts.append(ts_recovered)\n",
    "covid_19_ts = covid_19_ts.append(ts_confirmed)\n",
    "covid_19_ts = covid_19_ts.sort_values(['Country/Region', 'Province/State', 'Date']).reset_index(drop=True)\n",
    "\n",
    "#now drop 0 values\n",
    "covid_19_ts = covid_19_ts[covid_19_ts['Observation'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf.to_csv(output_dir + \"covid_19_by_date_and_country.csv\", index=False, encoding='utf-8-sig')\n",
    "covid_19_ts.to_csv(output_dir + \"covid_19_ts.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display for debug\n",
    "#display(covid_19_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sourcing from CDS here: https://coronadatascraper.com/#home\n",
    "# we really like these guys, but would recommend that you fork and spin up your own scraper set\n",
    "# set the following url to your own source\n",
    "scraper = pd.read_csv('https://coronadatascraper.com/timeseries.csv')\n",
    "scraper['date'] = pd.to_datetime(scraper['date']).dt.round(freq = 'D')\n",
    "scraper.to_csv(output_dir+'scraper_raw.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper['cases'] = scraper['cases'].fillna(0)\n",
    "scraper['recovered'] = scraper['recovered'].fillna(0)\n",
    "scraper['active'] = scraper['active'].fillna(0)\n",
    "scraper['tested'] = scraper['tested'].fillna(0)\n",
    "scraper['growthFactor'] = scraper['growthFactor'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper['Geo_Input'] = scraper['state']+', '+scraper['country'] \n",
    "non_located = scraper[scraper['lat'].isna()]\n",
    "geo_inputs = non_located['Geo_Input'].dropna().unique()\n",
    "scraper['Location_Key_Raw'] = scraper.apply(lambda x: (x.lat, x.long), axis = 1)\n",
    "scraper['Location_Key'] = scraper.apply(lambda x: (x.lat, x.long), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode_scraper():\n",
    "    print('Geocoding for: ', len(geo_inputs), 'locations')\n",
    "    d = dict(zip(geo_inputs, pd.Series(geo_inputs).apply(geocode).apply(lambda x: (x.latitude if pd.notnull(x.latitude) else x.latitude, \n",
    "                                                                                   x.longitude if pd.notnull(x.longitude) else x.longitude) if pd.notnull(x) else x)\n",
    "                )\n",
    "            )\n",
    "    pickle.dump(d, open('./reference/geoloc_dict_scraper.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geocode_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pickle.load(open('./reference/geoloc_dict_scraper.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper['Location_Key'] = scraper['Geo_Input'].map(d)\n",
    "scraper['Location_Key'] = scraper['Location_Key'].fillna(scraper['Location_Key_Raw'])\n",
    "scraper.loc[scraper['lat'].isna(), 'lat'] = scraper.loc[scraper['lat'].isna(), 'Location_Key'].apply(lambda x: (x[0] if pd.notnull(x[0]) else x[0]) if pd.notnull(x) else x)\n",
    "scraper.loc[scraper['lat'].isna(), 'long'] = scraper.loc[scraper['lat'].isna(), 'Location_Key'].apply(lambda x: (x[1] if pd.notnull(x[1]) else x[1]) if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revgeocode_scraper():\n",
    "    rev_set = scraper[['lat', 'long']].dropna().drop_duplicates()\n",
    "    print('Reverse geocoding for', rev_set.shape[0],'locations')\n",
    "    rev_list = rev_set['lat'].astype(str) + ', ' + rev_set['long'].astype(str)\n",
    "    r = rev_list.values\n",
    "    d = dict(zip(rev_list, pd.Series(r).apply(revgeocode).apply(lambda x: x if pd.notnull(x) else x)))\n",
    "    pickle.dump(d, open('./reference/geoloc_rev_dict_scraper.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revgeocode_scraper()./reference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = pd.read_csv('./reference/country_region_mappings.csv').set_index('alpha-3')['name']\n",
    "country_codes.name = 'country'\n",
    "# do some cleanup on this stuff\n",
    "country_codes.update(pd.Series({\n",
    "    'USA': 'USA',\n",
    "    'GBR': 'UK',\n",
    "    'KOR': 'South Korea',\n",
    "}))\n",
    "# now let's do some display friendly naming thatnks to Jason Curtis\n",
    "def get_combined_location(row):\n",
    "    location_segments = [\n",
    "        row['city'], row['county'], row['state'], row['country']\n",
    "    ]\n",
    "    cleaned_location_segments = [\n",
    "        segment\n",
    "        for segment in location_segments\n",
    "        if type(segment) is str\n",
    "    ]\n",
    "    return ', '.join(cleaned_location_segments)\n",
    "\n",
    "cleaned_timeseries = (\n",
    "    scraper.rename(\n",
    "        {\n",
    "            'country': 'country_code'\n",
    "        },\n",
    "        axis='columns'\n",
    "    ).join(country_codes, 'country_code')\n",
    ")\n",
    "\n",
    "cleaned_timeseries['location'] = cleaned_timeseries.apply(get_combined_location, axis='columns')\n",
    "#cleaned_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_timeseries.to_csv(output_dir+'scraper_cleaned.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now reshape and rename for backwards compat\n",
    "cleaned_timeseries['date'] = pd.to_datetime(cleaned_timeseries['date']).dt.round(freq = 'D')\n",
    "cleaned_timeseries['Last Update'] = cleaned_timeseries['date']\n",
    "scraper_df = cleaned_timeseries[['Last Update', 'date', 'lat', 'long', 'location', 'city', 'county', 'state', 'country', 'population', 'active', 'cases', 'deaths', 'recovered', 'tested', 'growthFactor']].copy()\n",
    "scraper_df.columns = ['Last Update', 'Date', 'Latitude', 'Longitude', 'Location', 'City', 'County', 'State', 'Country', 'Population', 'Active', 'Confirmed', 'Deaths', 'Recovered', 'Tested', 'Growth Factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_country(row):\n",
    "    location_segments = [\n",
    "        row['State'], row['Country']\n",
    "    ]\n",
    "    cleaned_location_segments = [\n",
    "        segment\n",
    "        for segment in location_segments\n",
    "        if type(segment) is str\n",
    "    ]\n",
    "    return ', '.join(cleaned_location_segments)\n",
    "\n",
    "scraper_df['State and Country'] = scraper_df.apply(get_state_country, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good, looks like only dupes are due to NaN on lat/lon - this can be corrected with better reverse geocoding\n",
    "#scraper_df[scraper_df[['Last Update', 'Latitude', 'Longitude']].duplicated()]\n",
    "#scraper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper_df['Confirmed Death Rate'] = scraper_df['Confirmed'] / scraper_df['Deaths']\n",
    "scraper_df['Confirmed per 100k capita'] = scraper_df['Confirmed'] / scraper_df['Population'] * 1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is once again some great work from Jason Curtis\n",
    "scraper_df['Date'] = scraper_df['Date'].apply(lambda date: date.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper_df = calc_timeseries_by_group(scraper_df, 'Country')\n",
    "scraper_df = calc_timeseries_by_group(scraper_df, 'State and Country')\n",
    "scraper_df = calc_timeseries_by_group(scraper_df, 'Location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper_df = scraper_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper_df.to_csv(output_dir+'scraper.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# scraper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('miniconda3': virtualenv)",
   "language": "python",
   "name": "python37664bitminiconda3virtualenve3b6ad2979204d799798e97f12a748c5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
