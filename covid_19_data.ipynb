{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "# for geocoding stuff\n",
    "import geopandas as gpd\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./data/\"\n",
    "input_dir = \"./csse_covid_19_data/csse_covid_19_daily_reports/\"\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob(input_dir+'*.{}'.format(extension))]\n",
    "\n",
    "# %% combine em up\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "combined_csv.to_csv(output_dir + \"covid_19_raw.csv\", index=False, encoding='utf-8-sig') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "geopy.geocoders.options.default_timeout = 30\n",
    "locator = Nominatim(user_agent=\"mesur.io\")\n",
    "geocode = RateLimiter(locator.geocode, min_delay_seconds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do a little data cleanup\n",
    "combined_csv['Country/Region'] = combined_csv['Country/Region'].str.strip()\n",
    "combined_csv['Country/Region'] = combined_csv['Country/Region'].replace('Korea, South', 'South Korea')\n",
    "combined_csv['Country/Region'] = combined_csv['Country/Region'].replace('Republic of Korea', 'South Korea')\n",
    "combined_csv['Country/Region'] = combined_csv['Country/Region'].replace('Iran (Islamic Republic of)', 'Iran')\n",
    "combined_csv['Country/Region'] = combined_csv['Country/Region'].replace('Mainland China', 'China')\n",
    "\n",
    "combined_csv['Country_Region'] = combined_csv['Country_Region'].str.strip()\n",
    "combined_csv['Country_Region'] = combined_csv['Country_Region'].replace('Korea, South', 'South Korea')\n",
    "combined_csv['Country_Region'] = combined_csv['Country_Region'].replace('Republic of Korea', 'South Korea')\n",
    "combined_csv['Country_Region'] = combined_csv['Country_Region'].replace('Iran (Islamic Republic of)', 'Iran')\n",
    "combined_csv['Country_Region'] = combined_csv['Country_Region'].replace('Mainland China', 'China')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined_csv.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = combined.groupby(['Country/Region', 'Province/State'])['Latitude', 'Longitude'].mean().reset_index()\n",
    "locations.columns = ['Country/Region', 'Province/State', 'Latitude_Lookup', 'Longitude_Lookup']\n",
    "\n",
    "combined = pd.merge(left=combined, right=locations, left_on=['Country/Region', 'Province/State'], right_on=['Country/Region', 'Province/State'], how='left')\n",
    "combined['Latitude'] = combined['Latitude'].fillna(combined['Latitude_Lookup'])\n",
    "combined['Longitude'] = combined['Longitude'].fillna(combined['Longitude_Lookup'])\n",
    "del combined['Latitude_Lookup'] \n",
    "del combined['Longitude_Lookup'] \n",
    "\n",
    "locations2 = combined.groupby(['Country/Region', 'Province/State'])['Lat', 'Long_'].mean().reset_index()\n",
    "locations2.columns = ['Country/Region', 'Province/State', 'Latitude_Lookup', 'Longitude_Lookup']\n",
    "\n",
    "combined = pd.merge(left=combined, right=locations2, left_on=['Country/Region', 'Province/State'], right_on=['Country/Region', 'Province/State'], how='left')\n",
    "combined['Latitude'] = combined['Latitude'].fillna(combined['Latitude_Lookup'])\n",
    "combined['Longitude'] = combined['Longitude'].fillna(combined['Longitude_Lookup'])\n",
    "del combined['Latitude_Lookup'] \n",
    "del combined['Longitude_Lookup'] \n",
    "\n",
    "combined['Last Update'] = combined['Last Update'].fillna(combined['Last_Update'])\n",
    "combined['Country/Region'] = combined['Country/Region'].fillna(combined['Country_Region'])\n",
    "combined['Province/State'] = combined['Province/State'].fillna(combined['Province_State'])\n",
    "combined['Province/State'] = combined['Province/State'].fillna(combined['Country/Region'])\n",
    "combined['Confirmed'] = combined['Confirmed'].fillna(0)\n",
    "combined['Deaths'] = combined['Deaths'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['Last Update'] = pd.to_datetime(combined['Last Update'])\n",
    "combined = combined.sort_values('Last Update').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['Geo_Input'] = combined['Province/State']+', '+combined['Country/Region'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_located = combined[combined['Latitude'].isna()]\n",
    "non_located = non_located[non_located['Province/State'] != 'Cruise Ship']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_inputs = non_located['Geo_Input'].unique()\n",
    "combined['Location_Key_Raw'] = combined.apply(lambda x: (x.Latitude, x.Longitude), axis = 1)\n",
    "#for testing you may want to trim this down a bit\n",
    "#geo_inputs = geo_inputs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode():\n",
    "    print('Geocoding for: ', len(geo_inputs), 'locations')\n",
    "    #use progress_apply() for interactive progress\n",
    "    d = dict(zip(geo_inputs, pd.Series(geo_inputs).apply(geocode).apply(lambda x: (x.latitude if pd.notnull(x.latitude) else x.latitude, \n",
    "                                                                                   x.longitude if pd.notnull(x.longitude) else x.longitude) if pd.notnull(x) else x)\n",
    "                )\n",
    "            )\n",
    "    pd.DataFrame.from_dict(d, orient=\"index\").to_csv('./reference/geoloc_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('./reference/geoloc_dict.json', index_col=0).to_dict(\"split\")\n",
    "d = dict(zip(d[\"index\"], d[\"data\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['Location_Key'] = combined['Geo_Input'].map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['Location_Key'] = combined['Location_Key'].fillna(combined['Location_Key_Raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['Latitude'] = combined.loc[combined['Latitude'].isna(), 'Location_Key'].apply(lambda x: x[0])\n",
    "combined['Longitude'] = combined.loc[combined['Longitude'].isna(), 'Location_Key'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's do a recovery est\n",
    "# first need the day of outbreak\n",
    "combined = combined.sort_values('Last Update').reset_index(drop=True)\n",
    "combined['Day'] = combined.groupby('Country/Region').cumcount()\n",
    "combined['DayLoc'] = combined.groupby(['Latitude','Longitude']).cumcount()\n",
    "combined['DayCountry'] = combined.groupby('Country/Region').cumcount()\n",
    "combined['DayCountryProvince'] = combined.groupby('Geo_Input').cumcount()\n",
    "\n",
    "combined['UnknownActive'] = combined['Confirmed'] - combined['Deaths']\n",
    "combined['RecoveredEst'] = np.floor(combined['UnknownActive'] * .14)\n",
    "combined['Recovered'] = combined['Recovered'].fillna(0)\n",
    "#hold on this for now, there is a formula that is curve based for this\n",
    "#combined.loc[combined['Recovered'] == 0, 'Recovered'] = combined['RecoveredEst']\n",
    "combined['Active'] = combined['UnknownActive'] - combined['Recovered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Last Update</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Province/State</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>UnknownActive</th>\n",
       "      <th>Active</th>\n",
       "      <th>Day</th>\n",
       "      <th>DayLoc</th>\n",
       "      <th>DayCountry</th>\n",
       "      <th>DayCountryProvince</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-22 17:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>China</td>\n",
       "      <td>Anhui</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2020-01-22 17:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>China</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2020-01-22 17:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>China</td>\n",
       "      <td>Chongqing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>30</td>\n",
       "      <td>34</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-22 17:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>China</td>\n",
       "      <td>Fujian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-22 17:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>China</td>\n",
       "      <td>Gansu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21279</th>\n",
       "      <td>2020-03-25 23:37:49</td>\n",
       "      <td>-31.875984</td>\n",
       "      <td>147.286949</td>\n",
       "      <td>Australia</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>341</td>\n",
       "      <td>3</td>\n",
       "      <td>341</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21280</th>\n",
       "      <td>2020-03-25 23:37:49</td>\n",
       "      <td>-30.534366</td>\n",
       "      <td>135.630121</td>\n",
       "      <td>Australia</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>342</td>\n",
       "      <td>3</td>\n",
       "      <td>342</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21281</th>\n",
       "      <td>2020-03-25 23:37:49</td>\n",
       "      <td>-25.230301</td>\n",
       "      <td>121.018725</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Western Australia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>343</td>\n",
       "      <td>3</td>\n",
       "      <td>343</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21282</th>\n",
       "      <td>2020-03-25 23:37:49</td>\n",
       "      <td>-22.164678</td>\n",
       "      <td>144.584490</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>435.0</td>\n",
       "      <td>344</td>\n",
       "      <td>3</td>\n",
       "      <td>344</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21278</th>\n",
       "      <td>2020-03-25 23:37:49</td>\n",
       "      <td>-19.851610</td>\n",
       "      <td>133.230337</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Northern Territory</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>340</td>\n",
       "      <td>3</td>\n",
       "      <td>340</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21286 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Last Update   Latitude   Longitude Country/Region  \\\n",
       "1     2020-01-22 17:00:00   0.000000    0.000000          China   \n",
       "21    2020-01-22 17:00:00   0.000000    0.000000          China   \n",
       "37    2020-01-22 17:00:00   0.000000    0.000000          China   \n",
       "2     2020-01-22 17:00:00   0.000000    0.000000          China   \n",
       "8     2020-01-22 17:00:00   0.000000    0.000000          China   \n",
       "...                   ...        ...         ...            ...   \n",
       "21279 2020-03-25 23:37:49 -31.875984  147.286949      Australia   \n",
       "21280 2020-03-25 23:37:49 -30.534366  135.630121      Australia   \n",
       "21281 2020-03-25 23:37:49 -25.230301  121.018725      Australia   \n",
       "21282 2020-03-25 23:37:49 -22.164678  144.584490      Australia   \n",
       "21278 2020-03-25 23:37:49 -19.851610  133.230337      Australia   \n",
       "\n",
       "           Province/State  FIPS Admin2  Confirmed  Deaths  Recovered  \\\n",
       "1                   Anhui   0.0      0        1.0     0.0        0.0   \n",
       "21                Beijing   0.0      0       14.0     0.0        0.0   \n",
       "37              Chongqing   0.0      0        6.0     0.0        0.0   \n",
       "2                  Fujian   0.0      0        1.0     0.0        0.0   \n",
       "8                   Gansu   0.0      0        0.0     0.0        0.0   \n",
       "...                   ...   ...    ...        ...     ...        ...   \n",
       "21279     New South Wales   0.0      0     1029.0     7.0        4.0   \n",
       "21280     South Australia   0.0      0      170.0     0.0        6.0   \n",
       "21281   Western Australia   0.0      0      175.0     1.0        1.0   \n",
       "21282          Queensland   0.0      0      443.0     0.0        8.0   \n",
       "21278  Northern Territory   0.0      0        6.0     0.0        0.0   \n",
       "\n",
       "       UnknownActive  Active  Day  DayLoc  DayCountry  DayCountryProvince  \n",
       "1                1.0     1.0    1       1           1                   0  \n",
       "21              14.0    14.0   18      21          18                   0  \n",
       "37               6.0     6.0   30      34          30                   0  \n",
       "2                1.0     1.0    2       2           2                   0  \n",
       "8                0.0     0.0    7       8           7                   0  \n",
       "...              ...     ...  ...     ...         ...                 ...  \n",
       "21279         1022.0  1018.0  341       3         341                  58  \n",
       "21280          170.0   164.0  342       3         342                  53  \n",
       "21281          174.0   173.0  343       3         343                  25  \n",
       "21282          443.0   435.0  344       3         344                  55  \n",
       "21278            6.0     6.0  340       3         340                  21  \n",
       "\n",
       "[21286 rows x 16 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do a little reording and subselection\n",
    "combined_csv = combined[['Last Update','Latitude','Longitude','Country/Region','Province/State','FIPS','Admin2',\n",
    "                         'Confirmed','Deaths','Recovered','UnknownActive', 'Active',\n",
    "                         'Day','DayLoc','DayCountry','DayCountryProvince']]\n",
    "combined_csv = combined_csv.sort_values(['Last Update','Latitude','Longitude','Country/Region','Province/State'])\n",
    "#combined_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv.to_csv(output_dir + \"combined.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_cases = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/web-data/data/cases.csv')\n",
    "web_cases_state = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/web-data/data/cases_state.csv')\n",
    "web_cases_country = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/web-data/data/cases_country.csv')\n",
    "web_cases_time = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/web-data/data/cases_time.csv')\n",
    "\n",
    "web_cases.to_csv(output_dir + \"web_cases.csv\", index=False, encoding='utf-8-sig')\n",
    "web_cases_state.to_csv(output_dir + \"web_cases_state.csv\", index=False, encoding='utf-8-sig')\n",
    "web_cases_country.to_csv(output_dir + \"web_cases_country.csv\", index=False, encoding='utf-8-sig')\n",
    "web_cases_time.to_csv(output_dir + \"web_cases_time.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_state_abbrev = {\n",
    "        'Alabama': 'AL',\n",
    "        'Alaska': 'AK',\n",
    "        'Arizona': 'AZ',\n",
    "        'Arkansas': 'AR',\n",
    "        'California': 'CA',\n",
    "        'Colorado': 'CO',\n",
    "        'Connecticut': 'CT',\n",
    "        'Delaware': 'DE',\n",
    "        'District of Columbia': 'D.C.',\n",
    "        'Florida': 'FL',\n",
    "        'Georgia': 'GA',\n",
    "        'Hawaii': 'HI',\n",
    "        'Idaho': 'ID',\n",
    "        'Illinois': 'IL',\n",
    "        'Indiana': 'IN',\n",
    "        'Iowa': 'IA',\n",
    "        'Kansas': 'KS',\n",
    "        'Kentucky': 'KY',\n",
    "        'Louisiana': 'LA',\n",
    "        'Maine': 'ME',\n",
    "        'Maryland': 'MD',\n",
    "        'Massachusetts': 'MA',\n",
    "        'Michigan': 'MI',\n",
    "        'Minnesota': 'MN',\n",
    "        'Mississippi': 'MS',\n",
    "        'Missouri': 'MO',\n",
    "        'Montana': 'MT',\n",
    "        'Nebraska': 'NE',\n",
    "        'Nevada': 'NV',\n",
    "        'New Hampshire': 'NH',\n",
    "        'New Jersey': 'NJ',\n",
    "        'New Mexico': 'NM',\n",
    "        'New York': 'NY',\n",
    "        'North Carolina': 'NC',\n",
    "        'North Dakota': 'ND',\n",
    "        'Northern Mariana Islands':'MP',\n",
    "        'Ohio': 'OH',\n",
    "        'Oklahoma': 'OK',\n",
    "        'Oregon': 'OR',\n",
    "        'Palau': 'PW',\n",
    "        'Pennsylvania': 'PA',\n",
    "        'Puerto Rico': 'PR',\n",
    "        'Rhode Island': 'RI',\n",
    "        'South Carolina': 'SC',\n",
    "        'South Dakota': 'SD',\n",
    "        'Tennessee': 'TN',\n",
    "        'Texas': 'TX',\n",
    "        'Utah': 'UT',\n",
    "        'Vermont': 'VT',\n",
    "        'Virgin Islands': 'VI',\n",
    "        'Virginia': 'VA',\n",
    "        'Washington': 'WA',\n",
    "        'West Virginia': 'WV',\n",
    "        'Wisconsin': 'WI',\n",
    "        'Wyoming': 'WY',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_csv.copy()\n",
    "df['Last Update'] = pd.to_datetime(df['Last Update']).dt.date \n",
    "\n",
    "def firsti(Series, offset):\n",
    "    return Series.first(offset)\n",
    "\n",
    "df = df.groupby(by=['Last Update', 'Country/Region'])[\n",
    "    #'Last Update', 'Country/Region',\n",
    "    'Confirmed', 'Deaths', 'Recovered'].sum().reset_index()\n",
    "df = df.sort_values('Last Update', ascending=True).reset_index()\n",
    "df['Active Cases'] = df['Confirmed'] - df['Recovered'] - df['Deaths']\n",
    "df['Cases'] = df['Confirmed'] - df['Recovered'] \n",
    "df['Death Rate'] = df['Deaths'] / df['Confirmed']\n",
    "df['Recovery Rate'] = df['Recovered'] / df['Confirmed']\n",
    "df['New Deaths'] = df['Deaths'] - df['Deaths'].shift()\n",
    "df['New Recovered'] = df['Recovered'] - df['Recovered'].shift()\n",
    "df['New Cases'] = df['Confirmed'] - df['Confirmed'].shift()\n",
    "df['New Case Rate'] = df['New Cases'].pct_change()\n",
    "df['New Death Rate'] = df['New Deaths'].pct_change()\n",
    "df['Last Update'] = pd.to_datetime(df['Last Update'])\n",
    "df['Date'] = pd.DatetimeIndex(df['Last Update']).astype ( np.int64 )/1000000\n",
    "df['Day'] = df.groupby('Country/Region').cumcount()\n",
    "df = df.dropna().reset_index()\n",
    "\n",
    "df.to_csv(output_dir + \"covid_19_by_date_and_country.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf = df.copy().groupby('Last Update').agg({\n",
    "    'Confirmed':'sum',\n",
    "    'Deaths':'sum',\n",
    "    'Recovered':'sum'\n",
    "    }).reset_index()\n",
    "overallDf = overallDf.sort_values('Last Update', ascending=True)\n",
    "overallDf['Active Cases'] = overallDf['Confirmed'] - overallDf['Recovered'] - overallDf['Deaths']\n",
    "overallDf['Cases'] = overallDf['Confirmed'] - overallDf['Recovered'] \n",
    "overallDf['Death Rate'] = overallDf['Deaths'] / overallDf['Confirmed']\n",
    "overallDf['Recovery Rate'] = overallDf['Recovered'] / overallDf['Confirmed']\n",
    "overallDf['New Deaths'] = overallDf['Deaths'] - overallDf['Deaths'].shift()\n",
    "overallDf['New Recovered'] = overallDf['Recovered'] - overallDf['Recovered'].shift()\n",
    "overallDf['New Cases'] = overallDf['Confirmed'] - overallDf['Confirmed'].shift()\n",
    "overallDf['New Case Rate'] = overallDf['New Cases'].pct_change()\n",
    "overallDf['New Death Rate'] = overallDf['New Deaths'].pct_change()\n",
    "overallDf['Date'] = pd.DatetimeIndex(overallDf['Last Update']).astype ( np.int64 )/1000000\n",
    "overallDf = overallDf.dropna().reset_index()\n",
    "\n",
    "overallDf.to_csv(output_dir + \"covid_19_by_date.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf = df.copy().groupby('Day').agg({\n",
    "    'Confirmed':'sum',\n",
    "    'Deaths':'sum',\n",
    "    'Recovered':'sum'\n",
    "    }).reset_index()\n",
    "overallDf = overallDf.sort_values('Day', ascending=True)\n",
    "overallDf['Active Cases'] = overallDf['Confirmed'] - overallDf['Recovered'] - overallDf['Deaths']\n",
    "overallDf['Cases'] = overallDf['Confirmed'] - overallDf['Recovered'] \n",
    "overallDf['Death Rate'] = overallDf['Deaths'] / overallDf['Confirmed']\n",
    "overallDf['Recovery Rate'] = overallDf['Recovered'] / overallDf['Confirmed']\n",
    "overallDf['New Deaths'] = overallDf['Deaths'] - overallDf['Deaths'].shift()\n",
    "overallDf['New Recovered'] = overallDf['Recovered'] - overallDf['Recovered'].shift()\n",
    "overallDf['New Cases'] = overallDf['Confirmed'] - overallDf['Confirmed'].shift()\n",
    "overallDf['New Case Rate'] = overallDf['New Cases'].pct_change()\n",
    "overallDf['New Death Rate'] = overallDf['New Deaths'].pct_change()\n",
    "overallDf = overallDf.dropna().reset_index()\n",
    "\n",
    "overallDf.to_csv(output_dir + \"covid_19_by_day.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf = df.copy().groupby(by=['Country/Region','Day']).agg({\n",
    "    'Confirmed':'sum',\n",
    "    'Deaths':'sum',\n",
    "    'Recovered':'sum'\n",
    "    }).reset_index()\n",
    "overallDf = overallDf.sort_values('Day', ascending=True)\n",
    "overallDf['Active Cases'] = overallDf['Confirmed'] - overallDf['Recovered'] - overallDf['Deaths']\n",
    "overallDf['Cases'] = overallDf['Confirmed'] - overallDf['Recovered'] \n",
    "overallDf['Death Rate'] = overallDf['Deaths'] / overallDf['Confirmed']\n",
    "overallDf['Recovery Rate'] = overallDf['Recovered'] / overallDf['Confirmed']\n",
    "overallDf['New Deaths'] = overallDf['Deaths'] - overallDf['Deaths'].shift()\n",
    "overallDf['New Recovered'] = overallDf['Recovered'] - overallDf['Recovered'].shift()\n",
    "overallDf['New Cases'] = overallDf['Confirmed'] - overallDf['Confirmed'].shift()\n",
    "overallDf['New Case Rate'] = overallDf['New Cases'].pct_change()\n",
    "overallDf['New Death Rate'] = overallDf['New Deaths'].pct_change()\n",
    "overallDf = overallDf.dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#john's hopkins raw files\n",
    "ts_deaths = pd.read_csv('csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')\n",
    "ts_confirmed = pd.read_csv('csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n",
    "#new recovered tracking has now been dropped :(\n",
    "ts_recovered = pd.read_csv('csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv')\n",
    "\n",
    "# let's unpivot that nasty excel style stuff\n",
    "ts_deaths = pd.melt(ts_deaths, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], var_name='Date', value_name='Observation')\n",
    "ts_deaths['Observation Type'] = 'Death'\n",
    "ts_confirmed = pd.melt(ts_confirmed, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], var_name='Date', value_name='Observation')\n",
    "ts_confirmed['Observation Type'] = 'Confirmed'\n",
    "ts_recovered = pd.melt(ts_recovered, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], var_name='Date', value_name='Observation')\n",
    "ts_recovered['Observation Type'] = 'Recovered'\n",
    "\n",
    "ts_deaths['Date'] = pd.to_datetime(ts_deaths['Date'])\n",
    "ts_confirmed['Date'] = pd.to_datetime(ts_confirmed['Date'])\n",
    "ts_recovered['Date'] = pd.to_datetime(ts_recovered['Date'])\n",
    "\n",
    "#and concat into one nice set\n",
    "covid_19_ts = ts_deaths.copy()\n",
    "covid_19_ts = covid_19_ts.append(ts_recovered)\n",
    "covid_19_ts = covid_19_ts.append(ts_confirmed)\n",
    "covid_19_ts = covid_19_ts.sort_values(['Country/Region', 'Province/State', 'Date']).reset_index(drop=True)\n",
    "\n",
    "#now drop 0 values\n",
    "covid_19_ts = covid_19_ts[covid_19_ts['Observation'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDf.to_csv(output_dir + \"covid_19_by_date_and_country.csv\", index=False, encoding='utf-8-sig')\n",
    "covid_19_ts.to_csv(output_dir + \"covid_19_ts.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display for debug\n",
    "#display(covid_19_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run any JH combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_19_by_country_and_day_of_outbreak = pd.read_csv(output_dir + 'covid_19_by_date_and_country.csv')\n",
    "covid_19_by_date = pd.read_csv(output_dir + 'covid_19_by_date.csv')\n",
    "covid_19_by_day = pd.read_csv(output_dir + 'covid_19_by_day.csv')\n",
    "covid_19_overall = pd.read_csv(output_dir + 'covid_19.csv')\n",
    "covid_19_ts = pd.read_csv(output_dir + 'covid_19_ts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_19_ts['Date'] = pd.to_datetime(covid_19_ts['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = covid_19_ts.copy()\n",
    "#if you just want to include some, exclude others, see line below:\n",
    "#ts_df = covid_19_ts[covid_19_ts['Country/Region'] != 'US'].copy()\n",
    "ts_df = ts_df.drop('Province/State', axis=1)\n",
    "ts_df = ts_df.sort_values(['Country/Region', 'Date'])\n",
    "\n",
    "covid_19_national_observations = ts_df.groupby(['Date', 'Country/Region', 'Observation Type'])['Observation'].sum().reset_index()\n",
    "covid_19_national_observations = covid_19_national_observations.pivot_table(columns='Observation Type', index=['Date', 'Country/Region'], values='Observation').reset_index().rename_axis(None, axis=1).fillna(0)\n",
    "\n",
    "#let's get a copy for post infection as well\n",
    "covid_19_infected_observations = covid_19_national_observations.copy()\n",
    "covid_19_infected_observations = covid_19_infected_observations[covid_19_infected_observations['Confirmed'] >= 20].fillna(0)\n",
    "\n",
    "#on to key values for reporting\n",
    "covid_19_national_observations['Day'] = covid_19_national_observations.groupby('Country/Region')['Confirmed'].cumcount()\n",
    "covid_19_national_observations['Day'] = covid_19_national_observations.groupby('Country/Region')['Confirmed'].fillna(method='bfill')\n",
    "\n",
    "covid_19_national_observations['Active Cases'] = covid_19_national_observations['Confirmed'] - covid_19_national_observations['Recovered']  - covid_19_national_observations['Death'] \n",
    "\n",
    "covid_19_national_observations['Likely Cases 1pct'] = covid_19_national_observations['Death'] * 100\n",
    "covid_19_national_observations['Likely Cases 1.8pct'] = covid_19_national_observations['Death'] * 180\n",
    "covid_19_national_observations['Likely Cases 3.5pct'] = covid_19_national_observations['Death'] * 350\n",
    "\n",
    "covid_19_national_observations['Death Rate'] = np.round(covid_19_national_observations['Death'] / covid_19_national_observations['Confirmed'],3)\n",
    "covid_19_national_observations['Death Change Rate'] = covid_19_national_observations.groupby('Country/Region')['Death'].pct_change()\n",
    "covid_19_national_observations['Recovery Change Rate'] = covid_19_national_observations.groupby('Country/Region')['Recovered'].pct_change()\n",
    "covid_19_national_observations['Confirmed Change Rate'] = covid_19_national_observations.groupby('Country/Region')['Confirmed'].pct_change()\n",
    "\n",
    "#now for same values on limited set\n",
    "covid_19_infected_observations['Day'] = covid_19_infected_observations.groupby('Country/Region', squeeze=True).cumcount()\n",
    "\n",
    "covid_19_infected_observations['Active Cases'] = covid_19_infected_observations['Confirmed'] - covid_19_infected_observations['Recovered']  - covid_19_infected_observations['Death'] \n",
    "\n",
    "covid_19_infected_observations['Likely Cases 1pct'] = covid_19_infected_observations['Death'] * 100\n",
    "covid_19_infected_observations['Likely Cases 1.8pct'] = covid_19_infected_observations['Death'] * 180\n",
    "covid_19_infected_observations['Likely Cases 3.5pct'] = covid_19_infected_observations['Death'] * 350\n",
    "\n",
    "covid_19_infected_observations['Death Rate'] = np.round(covid_19_infected_observations['Death'] / covid_19_infected_observations['Confirmed'],3)\n",
    "covid_19_infected_observations['Death Change Rate'] = covid_19_infected_observations.groupby('Country/Region')['Death'].pct_change()\n",
    "covid_19_infected_observations['Recovery Change Rate'] = covid_19_infected_observations.groupby('Country/Region')['Recovered'].pct_change()\n",
    "covid_19_infected_observations['Confirmed Change Rate'] = covid_19_infected_observations.groupby('Country/Region')['Confirmed'].pct_change()\n",
    "\n",
    "covid_19_infected_observations['Case Bins'] = pd.qcut(covid_19_infected_observations['Active Cases'], 10)\n",
    "maxes = covid_19_infected_observations.groupby('Country/Region')['Day'].max().reset_index()\n",
    "maxes.columns = ['Country/Region', 'Max Day']\n",
    "covid_19_infected_observations = pd.merge(left=covid_19_infected_observations, right=maxes, left_on='Country/Region', right_on='Country/Region', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_19_national_observations.to_csv(output_dir + \"global/covid_19_national_observations.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "covid_19_infected_observations.to_csv(output_dir + \"global/covid_19_infected_observations.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid_19_national_observations[covid_19_national_observations['Country/Region']=='Brazil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_totals = covid_19_national_observations.groupby(['Date']).sum().reset_index().copy()\n",
    "world_totals = world_totals.sort_values('Date')\n",
    "world_totals['Total Confirmed'] = world_totals['Confirmed'].rolling(1).sum().fillna(0)\n",
    "world_totals['Total Deaths'] = world_totals['Death'].rolling(1).sum().fillna(0)\n",
    "world_totals['Total Recovered'] = world_totals['Recovered'].rolling(1).sum().fillna(0)\n",
    "world_totals['Active Cases'] = world_totals['Total Confirmed'] - world_totals['Total Deaths'] - world_totals['Total Recovered']\n",
    "world_totals['Death Rate'] = np.round(world_totals['Total Deaths'] / world_totals['Total Confirmed'],3)\n",
    "world_totals['Death Change Rate'] = world_totals['Total Deaths'].pct_change()\n",
    "world_totals['Recovery Change Rate'] = world_totals['Total Recovered'].pct_change()\n",
    "world_totals['Confirmed Change Rate'] = world_totals['Total Confirmed'].pct_change()\n",
    "\n",
    "world_totals['Likely Cases C86'] = world_totals['Active Cases'] * 1.14\n",
    "world_totals['Likely Cases 1pct'] = world_totals['Total Deaths'] * 100\n",
    "world_totals['Likely Cases 1.8pct'] = world_totals['Total Deaths'] * 180\n",
    "world_totals['Likely Cases 3.5pct'] = world_totals['Total Deaths'] * 350\n",
    "\n",
    "world_totals.to_csv(output_dir + \"global/covid_19_world_totals.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country specific stuff below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanStr(s):\n",
    "    c = s.lower().replace(' ', '_')\n",
    "    c = c.replace('*', '').replace('(', '_').replace(')', '_').replace(',', '_')\n",
    "    c = c.strip()\n",
    "    return c\n",
    "\n",
    "def partitionByCountry(country):  \n",
    "    print('Processing data files for', country)\n",
    "    c = cleanStr(country)\n",
    "    os.makedirs(output_dir + 'countries/'+c, exist_ok=True)\n",
    "    \n",
    "    df = combined_csv[combined_csv['Country/Region'] == country].copy()\n",
    "    #add some backwards compat\n",
    "    df['Date'] = df['Last Update']\n",
    "    df['Death'] = df['Deaths']\n",
    "\n",
    "    df['Province/State'] = df['Province/State'].str.rsplit(',').str[-1].str.strip() \n",
    "    df['Province/State'] = df['Province/State'].replace(us_state_abbrev)\n",
    "\n",
    "    df = df.sort_values(['Province/State', 'Date'])\n",
    "    \n",
    "    if df['Province/State'].count() > 1:\n",
    "        print(df['Province/State'].unique())\n",
    "\n",
    "    cases = pd.DataFrame()\n",
    "    if df['Province/State'].count() == 0:\n",
    "        df['Province/State'] = country\n",
    "    cases = df.groupby(['Date', 'Province/State']).sum().reset_index()\n",
    "    \n",
    "    totals = cases.groupby(['Date']).sum().reset_index().copy()\n",
    "    totals = totals.sort_values('Date')    \n",
    "    \n",
    "    totals['Total Confirmed'] = totals['Confirmed'].rolling(1).sum().fillna(0)\n",
    "    totals['Total Deaths'] = totals['Death'].rolling(1).sum().fillna(0)\n",
    "    totals['Total Recovered'] = totals['Recovered'].rolling(1).sum().fillna(0)\n",
    "    totals['Active Cases'] = totals['Total Confirmed'] - totals['Total Deaths'] - totals['Total Recovered']\n",
    "    totals['Death Rate'] = np.round(totals['Total Deaths'] / totals['Total Confirmed'], 3).fillna(0)\n",
    "    totals['Death Change Rate'] = np.round(totals['Total Deaths'].pct_change(), 3).fillna(0)\n",
    "    totals['Recovery Change Rate'] = np.round(totals['Total Recovered'].pct_change(), 3).fillna(0)\n",
    "    totals['Confirmed Change Rate'] = np.round(totals['Total Confirmed'].pct_change(), 3).fillna(0)\n",
    "    totals['Confirmed Rolling 3 Change Rate'] = totals['Confirmed Change Rate'].rolling(3).mean().fillna(0)\n",
    "    \n",
    "    totals['New Active Cases'] = totals['Active Cases'] - totals['Active Cases'].shift()\n",
    "    totals['New Active Cases PCT Change'] = totals['New Active Cases'].pct_change().fillna(0)\n",
    "\n",
    "    totals['New Cases'] = totals['Confirmed'] - totals['Confirmed'].shift()\n",
    "    totals['New Case PCT Change'] = totals['New Cases'].pct_change().fillna(0)\n",
    "\n",
    "    totals['New Deaths'] = totals['Death'] - totals['Death'].shift()\n",
    "    totals['New Death PCT Change'] = totals['New Deaths'].pct_change().fillna(0)\n",
    "    \n",
    "    totals['New Recovered'] = totals['Recovered'] - totals['Recovered'].shift()\n",
    "    totals['New Recovered PCT Change'] = totals['New Recovered'].pct_change().fillna(0)\n",
    "    \n",
    "    totals['Likely Cases C86'] = totals['Active Cases'] * 1.14\n",
    "    totals['Likely Cases 1pct'] = totals['Total Deaths'] * 100\n",
    "    totals['Likely Cases 1.8pct'] = totals['Total Deaths'] * 180\n",
    "    totals['Likely Cases 3.5pct'] = totals['Total Deaths'] * 350\n",
    "    \n",
    "    if df['Province/State'].count() <= 1:\n",
    "        cases['Total Confirmed'] = cases['Confirmed'].rolling(1).sum()\n",
    "        cases['Total Deaths'] = cases['Death'].rolling(1).sum()\n",
    "        cases['Total Recovered'] = cases['Recovered'].rolling(1).sum()\n",
    "    else:\n",
    "        cases['Total Confirmed'] = cases.groupby('Province/State')['Confirmed'].rolling(1).sum().reset_index(0,drop=True)\n",
    "        cases['Total Deaths'] = cases.groupby('Province/State')['Death'].rolling(1).sum().reset_index(0,drop=True)\n",
    "        cases['Total Recovered'] = cases.groupby('Province/State')['Recovered'].rolling(1).sum().reset_index(0,drop=True)\n",
    "    cases['Active Cases'] = totals['Total Confirmed'] - totals['Total Deaths'] - totals['Total Recovered']\n",
    "    cases['Death Rate'] = np.round(totals['Total Deaths'] / totals['Total Confirmed'],3)\n",
    "    cases['Death Change Rate'] = np.round(totals['Total Deaths'].pct_change(), 3)\n",
    "    cases['Recovery Change Rate'] = np.round(totals['Total Recovered'].pct_change(), 3)\n",
    "    cases['Confirmed Change Rate'] = np.round(totals['Total Confirmed'].pct_change(), 3)\n",
    "\n",
    "    \n",
    "    cases['Likely Cases 1pct'] = np.round(cases['Death'] * 100)\n",
    "    cases['Likely Cases 1.8pct'] = np.round(cases['Death'] * 180)\n",
    "    cases['Likely Cases 3.5pct'] = np.round(cases['Death'] * 350)\n",
    "    cases['Likely Cases C86'] = np.round(cases['Confirmed'] * 1.14)\n",
    "\n",
    "        \n",
    "    cases.to_csv(output_dir + 'countries/'+c+'/covid_19_'+c+'_cases.csv', index=False, encoding='utf-8-sig')\n",
    "    totals.to_csv(output_dir + 'countries/'+c+'/covid_19_'+c+'_totals.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data files for Afghanistan\n",
      "['Afghanistan']\n",
      "Processing data files for Albania\n",
      "['Albania']\n",
      "Processing data files for Algeria\n",
      "['Algeria']\n",
      "Processing data files for Andorra\n",
      "['Andorra']\n",
      "Processing data files for Angola\n",
      "['Angola']\n",
      "Processing data files for Antigua and Barbuda\n",
      "['Antigua and Barbuda']\n",
      "Processing data files for Argentina\n",
      "['Argentina']\n",
      "Processing data files for Armenia\n",
      "['Armenia']\n",
      "Processing data files for Aruba\n",
      "['Aruba']\n",
      "Processing data files for Australia\n",
      "['Australia' 'Australian Capital Territory' 'External territories'\n",
      " 'From Diamond Princess' 'Jervis Bay Territory' 'New South Wales'\n",
      " 'Northern Territory' 'Queensland' 'South Australia' 'Tasmania' 'Victoria'\n",
      " 'Western Australia']\n",
      "Processing data files for Austria\n",
      "['Austria' 'None']\n",
      "Processing data files for Azerbaijan\n",
      "['Azerbaijan']\n",
      "Processing data files for Bahamas\n",
      "['Bahamas']\n",
      "Processing data files for Bahamas, The\n",
      "['The']\n",
      "Processing data files for Bahrain\n",
      "['Bahrain']\n",
      "Processing data files for Bangladesh\n",
      "['Bangladesh']\n",
      "Processing data files for Barbados\n",
      "['Barbados']\n",
      "Processing data files for Belarus\n",
      "['Belarus']\n",
      "Processing data files for Belgium\n",
      "['Belgium']\n",
      "Processing data files for Belize\n",
      "['Belize']\n",
      "Processing data files for Benin\n",
      "['Benin']\n",
      "Processing data files for Bhutan\n",
      "['Bhutan']\n",
      "Processing data files for Bolivia\n",
      "['Bolivia']\n",
      "Processing data files for Bosnia and Herzegovina\n",
      "['Bosnia and Herzegovina']\n",
      "Processing data files for Brazil\n",
      "['Brazil']\n",
      "Processing data files for Brunei\n",
      "['Brunei']\n",
      "Processing data files for Bulgaria\n",
      "['Bulgaria']\n",
      "Processing data files for Burkina Faso\n",
      "['Burkina Faso']\n",
      "Processing data files for Cabo Verde\n",
      "['Cabo Verde']\n",
      "Processing data files for Cambodia\n",
      "['Cambodia']\n",
      "Processing data files for Cameroon\n",
      "['Cameroon']\n",
      "Processing data files for Canada\n",
      "['Alberta' 'British Columbia' 'Diamond Princess' 'Grand Princess'\n",
      " 'Manitoba' 'New Brunswick' 'Newfoundland and Labrador'\n",
      " 'Northwest Territories' 'Nova Scotia' 'ON' 'Ontario'\n",
      " 'Prince Edward Island' 'QC' 'Quebec' 'Recovered' 'Saskatchewan']\n",
      "Processing data files for Cape Verde\n",
      "Processing data files for Cayman Islands\n",
      "['Cayman Islands']\n",
      "Processing data files for Central African Republic\n",
      "['Central African Republic']\n",
      "Processing data files for Chad\n",
      "['Chad']\n",
      "Processing data files for Channel Islands\n",
      "Processing data files for Chile\n",
      "['Chile']\n",
      "Processing data files for China\n",
      "['Anhui' 'Beijing' 'Chongqing' 'Fujian' 'Gansu' 'Guangdong' 'Guangxi'\n",
      " 'Guizhou' 'Hainan' 'Hebei' 'Heilongjiang' 'Henan' 'Hong Kong' 'Hubei'\n",
      " 'Hunan' 'Inner Mongolia' 'Jiangsu' 'Jiangxi' 'Jilin' 'Liaoning' 'Macau'\n",
      " 'Ningxia' 'Qinghai' 'Shaanxi' 'Shandong' 'Shanghai' 'Shanxi' 'Sichuan'\n",
      " 'Tianjin' 'Tibet' 'Xinjiang' 'Yunnan' 'Zhejiang']\n",
      "Processing data files for Colombia\n",
      "['Colombia']\n",
      "Processing data files for Congo (Brazzaville)\n",
      "['Congo (Brazzaville)']\n",
      "Processing data files for Congo (Kinshasa)\n",
      "['Congo (Kinshasa)']\n",
      "Processing data files for Costa Rica\n",
      "['Costa Rica']\n",
      "Processing data files for Cote d'Ivoire\n",
      "[\"Cote d'Ivoire\"]\n",
      "Processing data files for Croatia\n",
      "['Croatia']\n",
      "Processing data files for Cruise Ship\n",
      "['Diamond Princess']\n",
      "Processing data files for Cuba\n",
      "['Cuba']\n",
      "Processing data files for Curacao\n",
      "['Curacao']\n",
      "Processing data files for Cyprus\n",
      "['Cyprus']\n",
      "Processing data files for Czech Republic\n",
      "['Czech Republic']\n",
      "Processing data files for Czechia\n",
      "['Czechia']\n",
      "Processing data files for Denmark\n",
      "['Denmark' 'Faroe Islands' 'Greenland']\n",
      "Processing data files for Diamond Princess\n",
      "Processing data files for Djibouti\n",
      "['Djibouti']\n",
      "Processing data files for Dominica\n",
      "['Dominica']\n",
      "Processing data files for Dominican Republic\n",
      "['Dominican Republic']\n",
      "Processing data files for East Timor\n",
      "Processing data files for Ecuador\n",
      "['Ecuador']\n",
      "Processing data files for Egypt\n",
      "['Egypt']\n",
      "Processing data files for El Salvador\n",
      "['El Salvador']\n",
      "Processing data files for Equatorial Guinea\n",
      "['Equatorial Guinea']\n",
      "Processing data files for Eritrea\n",
      "['Eritrea']\n",
      "Processing data files for Estonia\n",
      "['Estonia']\n",
      "Processing data files for Eswatini\n",
      "['Eswatini']\n",
      "Processing data files for Ethiopia\n",
      "['Ethiopia']\n",
      "Processing data files for Faroe Islands\n",
      "['Faroe Islands']\n",
      "Processing data files for Fiji\n",
      "['Fiji']\n",
      "Processing data files for Finland\n",
      "['Finland']\n",
      "Processing data files for France\n",
      "['Fench Guiana' 'France' 'French Guiana' 'French Polynesia' 'Guadeloupe'\n",
      " 'Martinique' 'Mayotte' 'New Caledonia' 'Reunion' 'Saint Barthelemy'\n",
      " 'St Martin']\n",
      "Processing data files for French Guiana\n",
      "['French Guiana']\n",
      "Processing data files for Gabon\n",
      "['Gabon']\n",
      "Processing data files for Gambia\n",
      "['Gambia']\n",
      "Processing data files for Gambia, The\n",
      "['The']\n",
      "Processing data files for Georgia\n",
      "['GA']\n",
      "Processing data files for Germany\n",
      "['Bavaria' 'Germany']\n",
      "Processing data files for Ghana\n",
      "['Ghana']\n",
      "Processing data files for Gibraltar\n",
      "['Gibraltar']\n",
      "Processing data files for Greece\n",
      "['Greece']\n",
      "Processing data files for Greenland\n",
      "['Greenland']\n",
      "Processing data files for Grenada\n",
      "['Grenada']\n",
      "Processing data files for Guadeloupe\n",
      "['Guadeloupe']\n",
      "Processing data files for Guam\n",
      "['Guam']\n",
      "Processing data files for Guatemala\n",
      "['Guatemala']\n",
      "Processing data files for Guernsey\n",
      "['Guernsey']\n",
      "Processing data files for Guinea\n",
      "['Guinea']\n",
      "Processing data files for Guinea-Bissau\n",
      "Processing data files for Guyana\n",
      "['Guyana']\n",
      "Processing data files for Haiti\n",
      "['Haiti']\n",
      "Processing data files for Holy See\n",
      "['Holy See']\n",
      "Processing data files for Honduras\n",
      "['Honduras']\n",
      "Processing data files for Hong Kong\n",
      "['Hong Kong']\n",
      "Processing data files for Hong Kong SAR\n",
      "Processing data files for Hungary\n",
      "['Hungary']\n",
      "Processing data files for Iceland\n",
      "['Iceland']\n",
      "Processing data files for India\n",
      "['India']\n",
      "Processing data files for Indonesia\n",
      "['Indonesia']\n",
      "Processing data files for Iran\n",
      "['Iran']\n",
      "Processing data files for Iraq\n",
      "['Iraq' 'None']\n",
      "Processing data files for Ireland\n",
      "['Ireland']\n",
      "Processing data files for Israel\n",
      "['From Diamond Princess' 'Israel']\n",
      "Processing data files for Italy\n",
      "['Italy']\n",
      "Processing data files for Ivory Coast\n",
      "Processing data files for Jamaica\n",
      "['Jamaica']\n",
      "Processing data files for Japan\n",
      "['Japan']\n",
      "Processing data files for Jersey\n",
      "['Jersey']\n",
      "Processing data files for Jordan\n",
      "['Jordan']\n",
      "Processing data files for Kazakhstan\n",
      "['Kazakhstan']\n",
      "Processing data files for Kenya\n",
      "['Kenya']\n",
      "Processing data files for Kosovo\n",
      "['Kosovo']\n",
      "Processing data files for Kuwait\n",
      "['Kuwait']\n",
      "Processing data files for Kyrgyzstan\n",
      "['Kyrgyzstan']\n",
      "Processing data files for Laos\n",
      "['Laos']\n",
      "Processing data files for Latvia\n",
      "['Latvia']\n",
      "Processing data files for Lebanon\n",
      "['Lebanon' 'None']\n",
      "Processing data files for Liberia\n",
      "['Liberia']\n",
      "Processing data files for Libya\n",
      "['Libya']\n",
      "Processing data files for Liechtenstein\n",
      "['Liechtenstein']\n",
      "Processing data files for Lithuania\n",
      "['Lithuania']\n",
      "Processing data files for Luxembourg\n",
      "['Luxembourg']\n",
      "Processing data files for Macao SAR\n",
      "Processing data files for Macau\n",
      "['Macau']\n",
      "Processing data files for Madagascar\n",
      "['Madagascar']\n",
      "Processing data files for Malaysia\n",
      "['Malaysia']\n",
      "Processing data files for Maldives\n",
      "['Maldives']\n",
      "Processing data files for Mali\n",
      "Processing data files for Malta\n",
      "['Malta']\n",
      "Processing data files for Martinique\n",
      "['Martinique']\n",
      "Processing data files for Mauritania\n",
      "['Mauritania']\n",
      "Processing data files for Mauritius\n",
      "['Mauritius']\n",
      "Processing data files for Mayotte\n",
      "['Mayotte']\n",
      "Processing data files for Mexico\n",
      "['Mexico']\n",
      "Processing data files for Moldova\n",
      "['Moldova']\n",
      "Processing data files for Monaco\n",
      "['Monaco']\n",
      "Processing data files for Mongolia\n",
      "['Mongolia']\n",
      "Processing data files for Montenegro\n",
      "['Montenegro']\n",
      "Processing data files for Morocco\n",
      "['Morocco']\n",
      "Processing data files for Mozambique\n",
      "['Mozambique']\n",
      "Processing data files for Namibia\n",
      "['Namibia']\n",
      "Processing data files for Nepal\n",
      "['Nepal']\n",
      "Processing data files for Netherlands\n",
      "['Aruba' 'Curacao' 'Netherlands' 'Sint Maarten']\n",
      "Processing data files for New Zealand\n",
      "['New Zealand']\n",
      "Processing data files for Nicaragua\n",
      "['Nicaragua']\n",
      "Processing data files for Niger\n",
      "['Niger']\n",
      "Processing data files for Nigeria\n",
      "['Nigeria']\n",
      "Processing data files for North Ireland\n",
      "Processing data files for North Macedonia\n",
      "['North Macedonia']\n",
      "Processing data files for Norway\n",
      "['Norway']\n",
      "Processing data files for Oman\n",
      "['Oman']\n",
      "Processing data files for Others\n",
      "['Cruise Ship' 'Diamond Princess cruise ship']\n",
      "Processing data files for Pakistan\n",
      "['Pakistan']\n",
      "Processing data files for Palestine\n",
      "['Palestine']\n",
      "Processing data files for Panama\n",
      "['Panama']\n",
      "Processing data files for Papua New Guinea\n",
      "['Papua New Guinea']\n",
      "Processing data files for Paraguay\n",
      "['Paraguay']\n",
      "Processing data files for Peru\n",
      "['Peru']\n",
      "Processing data files for Philippines\n",
      "['Philippines']\n",
      "Processing data files for Poland\n",
      "['Poland']\n",
      "Processing data files for Portugal\n",
      "['Portugal']\n",
      "Processing data files for Puerto Rico\n",
      "['PR']\n",
      "Processing data files for Qatar\n",
      "['Qatar']\n",
      "Processing data files for Republic of Ireland\n",
      "Processing data files for Republic of Moldova\n",
      "Processing data files for Republic of the Congo\n",
      "['Republic of the Congo']\n",
      "Processing data files for Reunion\n",
      "['Reunion']\n",
      "Processing data files for Romania\n",
      "['Romania']\n",
      "Processing data files for Russia\n",
      "['Russia']\n",
      "Processing data files for Russian Federation\n",
      "Processing data files for Rwanda\n",
      "['Rwanda']\n",
      "Processing data files for Saint Barthelemy\n",
      "['Saint Barthelemy']\n",
      "Processing data files for Saint Kitts and Nevis\n",
      "Processing data files for Saint Lucia\n",
      "['Saint Lucia']\n",
      "Processing data files for Saint Martin\n",
      "Processing data files for Saint Vincent and the Grenadines\n",
      "['Saint Vincent and the Grenadines']\n",
      "Processing data files for San Marino\n",
      "['San Marino']\n",
      "Processing data files for Saudi Arabia\n",
      "['Saudi Arabia']\n",
      "Processing data files for Senegal\n",
      "['Senegal']\n",
      "Processing data files for Serbia\n",
      "['Serbia']\n",
      "Processing data files for Seychelles\n",
      "['Seychelles']\n",
      "Processing data files for Singapore\n",
      "['Singapore']\n",
      "Processing data files for Slovakia\n",
      "['Slovakia']\n",
      "Processing data files for Slovenia\n",
      "['Slovenia']\n",
      "Processing data files for Somalia\n",
      "['Somalia']\n",
      "Processing data files for South Africa\n",
      "['South Africa']\n",
      "Processing data files for South Korea\n",
      "['South Korea']\n",
      "Processing data files for Spain\n",
      "['Spain']\n",
      "Processing data files for Sri Lanka\n",
      "['Sri Lanka']\n",
      "Processing data files for St. Martin\n",
      "Processing data files for Sudan\n",
      "['Sudan']\n",
      "Processing data files for Suriname\n",
      "['Suriname']\n",
      "Processing data files for Sweden\n",
      "['Sweden']\n",
      "Processing data files for Switzerland\n",
      "['Switzerland']\n",
      "Processing data files for Syria\n",
      "['Syria']\n",
      "Processing data files for Taipei and environs\n",
      "Processing data files for Taiwan\n",
      "['Taiwan']\n",
      "Processing data files for Taiwan*\n",
      "['Taiwan*']\n",
      "Processing data files for Tanzania\n",
      "['Tanzania']\n",
      "Processing data files for Thailand\n",
      "['Thailand']\n",
      "Processing data files for The Bahamas\n",
      "['The Bahamas']\n",
      "Processing data files for The Gambia\n",
      "['The Gambia']\n",
      "Processing data files for Timor-Leste\n",
      "['Timor-Leste']\n",
      "Processing data files for Togo\n",
      "['Togo']\n",
      "Processing data files for Trinidad and Tobago\n",
      "['Trinidad and Tobago']\n",
      "Processing data files for Tunisia\n",
      "['Tunisia']\n",
      "Processing data files for Turkey\n",
      "['Turkey']\n",
      "Processing data files for UK\n",
      "['UK']\n",
      "Processing data files for US\n",
      "['AK' 'AL' 'AR' 'AZ' 'American Samoa' 'CA' 'CA (From Diamond Princess)'\n",
      " 'CO' 'CT' 'Chicago' 'D.C.' 'DE' 'Diamond Princess' 'FL' 'GA'\n",
      " 'Grand Princess' 'Grand Princess Cruise Ship' 'Guam' 'HI' 'IA' 'ID' 'IL'\n",
      " 'IN' 'KS' 'KY' 'LA' 'MA' 'MD' 'ME' 'MI' 'MN' 'MO' 'MP' 'MS' 'MT' 'NC'\n",
      " 'ND' 'NE' 'NE (From Diamond Princess)' 'NH' 'NJ' 'NM' 'NV' 'NY' 'OH' 'OK'\n",
      " 'OR' 'PA' 'PR' 'RI' 'Recovered' 'SC' 'SD' 'TN' 'TX'\n",
      " 'TX (From Diamond Princess)' 'U.S.' 'US' 'UT'\n",
      " 'Unassigned Location (From Diamond Princess)'\n",
      " 'United States Virgin Islands' 'VA' 'VI' 'VT' 'WA' 'WI' 'WV' 'WY'\n",
      " 'Wuhan Evacuee']\n",
      "Processing data files for Uganda\n",
      "['Uganda']\n",
      "Processing data files for Ukraine\n",
      "['Ukraine']\n",
      "Processing data files for United Arab Emirates\n",
      "['United Arab Emirates']\n",
      "Processing data files for United Kingdom\n",
      "['Bermuda' 'Cayman Islands' 'Channel Islands' 'Gibraltar' 'Isle of Man'\n",
      " 'Montserrat' 'UK' 'United Kingdom']\n",
      "Processing data files for Uruguay\n",
      "['Uruguay']\n",
      "Processing data files for Uzbekistan\n",
      "['Uzbekistan']\n",
      "Processing data files for Vatican City\n",
      "['Vatican City']\n",
      "Processing data files for Venezuela\n",
      "['Venezuela']\n",
      "Processing data files for Viet Nam\n",
      "Processing data files for Vietnam\n",
      "['Vietnam']\n",
      "Processing data files for Zambia\n",
      "['Zambia']\n",
      "Processing data files for Zimbabwe\n",
      "['Zimbabwe']\n",
      "Processing data files for occupied Palestinian territory\n",
      "['occupied Palestinian territory']\n"
     ]
    }
   ],
   "source": [
    "countries = combined_csv['Country/Region'].unique()\n",
    "\n",
    "countries.sort()\n",
    "for cnt in countries:\n",
    "    partitionByCountry(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('miniconda3': virtualenv)",
   "language": "python",
   "name": "python37664bitminiconda3virtualenve3b6ad2979204d799798e97f12a748c5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
